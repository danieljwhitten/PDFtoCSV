{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide for Installation and Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "### Anaconda & Jupyter Notebooks\n",
    "We recommend using this program via the [Anaconda Data Science Platform](https://www.anaconda.com/). This includes Jupyter Notebooks, and you likely already have it installed if you are viewing this file.  \n",
    "This program can also be run via any Python interpreter, including from the command line, if you have the technical knowledge to do so. \n",
    "\n",
    "### Python 3\n",
    "This program is written using the latest version of Python to date. To ensure you are ready to run it, ensure that the top right corner of your Jupyter Notebook says `Python 3`. If it does not, try to change it by going to the `Kernel` menu, choosing `Change Kernel`, then `Python 3`. If you do not see this option, you may need to download the latest version of Anaconda [here](https://www.anaconda.com/distribution/).  \n",
    "\n",
    "**With that, you should be able to follow the steps below and get going within a few minutes!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "The following steps are required before the program is run for the first time. They do not need to be repeated again on the same computer.\n",
    "\n",
    "### 1. Install Tesseract\n",
    "Tesseract is the tool this program uses to read text from pages that do not have machine-readable text. It is is a free, open-source program (sponsored by Google) that performs Optical Character Recognition (OCR). It must be installed first for this program to work properly. More information about Tesseract OCR can be found [here](https://github.com/tesseract-ocr/tesseract/blob/master/README.md).\n",
    "#### Windows\n",
    "1. Download the Tesseract OCR installer [here](https://github.com/UB-Mannheim/tesseract/wiki).  \n",
    "2. Run the .exe file once it has been downloaded.  \n",
    "3. Accept all default options.  \n",
    "\n",
    "#### Mac\n",
    "The easiest way is to install via [Homebrew](https://brew.sh/), a handy installer that works to easily install all kinds of usefuly programs, in addition to Tesseract OCR.  \n",
    "  * Open Terminal (Press Command + Space, type \"Terminal\", press Enter)  \n",
    "  * Paste: ```/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"```  \n",
    "  * Press Enter  \n",
    "  * Press Enter again  \n",
    "  * Enter your computer password to authorize installation when prompted  \n",
    "  * Homebrew has successfully installed once you  see `==> Installation successful!`  \n",
    "  * Paste the following: `brew install tesseract`  \n",
    "  * Press Enter  \n",
    "\n",
    "For experienced users, see [here](https://github.com/tesseract-ocr/tesseract/wiki#macos) for other installation options.  \n",
    "\n",
    "#### Linux\n",
    "If your distribution uses `apt`, like Ubuntu, the following should install Tesseract OCR:  \n",
    "`sudo apt install tesseract-ocr`  \n",
    "See [here](https://github.com/tesseract-ocr/tesseract/wiki#linux) for more detailed instructions for a wide variety of distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below installs the Python programs required for this program to run.  \n",
    "Execute the cell by clicking on it and pressing Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"installDependencies.py\"\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output above indicates `All packages are installed`, you are good to start using the program!  \n",
    "\n",
    "If not, try opening your command line (Windows: Open the program \"Command Prompt\", Mac: Open the program \"Terminal\") and paste the following, then press Enter: `python -m pip install unidecode pytesseract PyMuPDF natsort`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use the Program\n",
    "\n",
    "### Prepare Your PDF Input\n",
    "\n",
    "#### Legible Black and White Typed Text\n",
    "It works best with PDF files that have clearly legible typed black text on a white background. The majority of the text should be in the same font and the same size. The program will still try to process files with coloured backgrounds or various non-standard fonts, but the quality of the output may be low.\n",
    "\n",
    "#### Clearly Labelled Files\n",
    "The output will be one single CSV file (can be opened as a spreadsheet in Microsoft Excel). The text from each page of each document will be included on a separate line. Each line will have:\n",
    "1. The path of the source file, including preceding folders and the name of the file\n",
    "  * Eg. \"path/to/folder/with/PDF/2019/January/1Jan2019.pdf\"\n",
    "2. The page number, relative to each file\n",
    "3. The number of words captured from the page\n",
    "4. The text of the page  \n",
    "\n",
    "Because of this format, it is most convenient for you to take the time to name your folders and files descriptively. The program processes the files in the same order that they appear on your computer, when sorted by file name. However, if your folders are randomly or inconsistantly named, it will make it harder to sort and analyze the data\n",
    "                \n",
    "\n",
    "#### Awareness of Time Required\n",
    "This program extracts text in two different ways. First, it tries to see if any text is already embedded in the PDF (i.e., the PDF is \"machine readable\"). If there is no text embedded, the program then uses [Tesseract OCR](https://github.com/UB-Mannheim/tesseract/wiki) to read the letters on the page as plain text. It is not crucial to understand how this works, but it will make a different for how long the file(s) will take to process.\n",
    "* Machine readable PDF: ~0.009 seconds/page\n",
    "* Non-machine readable: ~1 second/100 words on page\n",
    "\n",
    "While this does not make a large difference if the file has only a few pages, the time difference increases with larger files, and can take days for 20,000+ pages. In order to accurately estimate the time required, open up your PDF files and determine whether they are machine readable or not. The easiest way is to search for a word you know is in the file. If your PDF reader finds the word, it has \"read\" the file, and the file is machine readable. If it does not, it is likely not machine readanble, and will require OCR. Another way of checking is to try to highligh the text with your mouse, if it highlights the lines of text, the page is likely machine readable. If it highlights the entire page as one large box, it is likely not.  \n",
    "For larger files, it is common to have some non-machine readable files distributed within machine readable pages. This is common with advertisements in newspapers, for example. The program will process any machine readble file accordingly, and only use OCR on individual pages when necessary, so such file should still be processed relatively quickly.  \n",
    "In order to optimize the speed of processing, minimize other tasks being performed by your computer at the same time. Computers have limited processing resources, so watching videos, surfing the internet, or writing in a word processor will all negatively impact the processing time for your files. This is not to say that one cannot multitask while processing files, and the effect is only noticable on large batches of non-machine readable files that are continually processing for hours or days at a time.\n",
    "\n",
    "### Run the Program\n",
    "\n",
    "#### In Jupyter Notebooks\n",
    "In Jupyter Notebooks, you can either use the cell below (Under \"Ready to Go!\"), or navigate to the folder where you downloaded this program and create a new notebook.  \n",
    "In a new cell, paste: `%run PDFtoCSV.py \"path/to/folder/with/PDF\"`\n",
    "\n",
    "#### From Command Line\n",
    "If you do not want to run the program via Jupyter Notebooks, it is possible to run via the command line (Command Prompt or Terminal), by running the following command `python path/to/download/PDFtoCSV.py /path/to/folder/with/PDFs`, replacing the first path with the location of this program and the second with the location of your PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to start extracting text from your PDF files!  \n",
    "The simplist way is to use the cell below. Simply replace the section in quotation marks with the path to the file or folder you want to convert. For instructions on copying and pasting paths, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run PDFtoCSV.py \"example/path/to/folder/with/PDFs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Just in case you accidentally erased the command above, here it is again for you to copy and paste:_  \n",
    "`%run PDFtoCSV.py \"example/path/to/folder/with/PDFs\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Options\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is designed to be simple to use, so it will run normally with the command [above](#Ready-to-Go!). However, there are a variety of options that can be used to customize the operation and output of the program to suit a variety of needs. These are activated by adding \"flags\" to the command. Flags are added by including a hyphen `-` followed by one or two letters after the path. Alternatively, these can also be added by including two hypehns `--` followed by the extended flag. The detailed flags for each option are explained below. \n",
    "\n",
    "For example, to show detailed progress on file processing (verbose mode), you would add `-v` to the line above, making it `%run PDFtoCSV.py \"example/path/to/folder/with/PDFs\" -v`. Note that multiple flags can be added at once, although some contradictory commands cannot be included at the same time. These will be noted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed and Detail Options\n",
    "\n",
    "#### Accelerated Mode\n",
    "`-a` or  `--accelerated`\n",
    "\n",
    "_Cannot be used at the same time as -t (Thorough Mode)_\n",
    "\n",
    "This option greatly increases the speed at which the program processes files, at the expense of comprehenive detail. As detailed above, some files contain machine-readable text, which the program reads at extremely high speeds. Other files contain text that that is stored as a picture. This can be read using **O**ptical **C**haracter **R**ecognition (OCR), which takes much longer to process. This mode ignores all OCR text and only processes machine-readable text. Depending on the details of the source PDF files, this could either have a very small or very large effect on the amount of text extracted.\n",
    "\n",
    "#### Thorough Mode\n",
    "`-t` or `--thorough`\n",
    "\n",
    "_Cannot be used at the same time as -a (Accelerated Mode)_\n",
    "\n",
    "\n",
    "This option extracts as much text as possible from each page, at the expense of a slower processing time. It examines each element on each page and checks all images to see if they contain readable text. During default mode, there is a potential pitfall where OCR text can be overlooked if there is also machine-readable text on the page. For example, if a page has a typed header and footed, but the content is a scanned image, the default mode will process the machine-readable header and footer and ignore the scanned image, as it has determined that the page is machine-readable. Thorough mode will process all elements on the page. The tradeoff for this comprehenisive analysis is a much slower processing speed for files that contain mostly machine-readable text, as these would be processed very quickly in default mode.\n",
    "\n",
    "_If certain pages or sections are being overlooked because they contain OCR and machine-readable content, it is recommended that those pages or sections be processed separately with thorough mode, and the CSV content combined afterwards._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Output Option\n",
    "\n",
    "#### Split Mode\n",
    "`-s` or `--split`\n",
    "This option creates a separate output CSV for each input file. For example, consider the following example directory:\n",
    "```\n",
    "PDFs/\n",
    "    file1.pdf\n",
    "    file2.pdf\n",
    "    more PDFs/\n",
    "        file3.pdf\n",
    "        file4.pdf\n",
    "```\n",
    "By default, the program creates one CSV output file with all of the contents of all files.\n",
    "`%run PDFtoCSV.py \"path/to/PDFs\"`\n",
    "```\n",
    "PDFs.csv\n",
    "PDFs/\n",
    "    file1.pdf\n",
    "    file2.pdf\n",
    "    more PDFs/\n",
    "        file3.pdf\n",
    "        file4.pdf\n",
    "```\n",
    "By using split mode, one CSV output file is generated for each individual input file.\n",
    "`%run PDFtoCSV.py \"path/to/PDFs\" -s`\n",
    "```\n",
    "PDFs/\n",
    "    file1.csv\n",
    "    file1.pdf\n",
    "    file2.csv\n",
    "    file2.pdf\n",
    "    more PDFs/\n",
    "        file3.csv\n",
    "        file3.pdf\n",
    "        file4.csv\n",
    "        file4.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Processing\n",
    "\n",
    "#### Pages Option\n",
    "`-p PAGES` or `--pages PAGES`\n",
    "\n",
    "This option only processes the contents of the given pages in each file. After the `-p` flag, `PAGES` above should be replaced by the list of pages. Pages can be listed indivuidually or in ranges in the form `1-10`. Multiple pages and ranges should be separated by a single space.\n",
    "\n",
    "For example, `%run PDFtoCSV.py \"example/path/to/folder/with/PDFs\" -p 1 2 11-14` will process pages 1, 2, 11, 12, 13, and 14 from each file in the folder `PDFs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress Update Options\n",
    "\n",
    "#### Quiet Mode\n",
    "`-q` or `--quiet`\n",
    "\n",
    "_Cannot be used at the same time as -v (Verbose Mode)_\n",
    "\n",
    "This option supresses updates about the progress of text extraction per file. Updates will only be given when the program starts and when it completes all files. Please note that files containing non-machine-readable text (OCR) can require an extraordinarily long time to complete, and the program may appear unresponsive while processing these in quiet mode.\n",
    "\n",
    "#### Verbose Mode\n",
    "`-v` or `--verbose`\n",
    "\n",
    "_Cannot be used at the same time as -q (Quiet Mode)_\n",
    "\n",
    "\n",
    "This option provides detailed updates for each page updated. This can be useful for tracking the progress of files, but can also result in a large amount of progress information being output in a short period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Customization Options\n",
    "\n",
    "#### Custom Field Arrangement\n",
    "`-f FIELDS` or `--fields FIELDS`\n",
    "\n",
    "This option allows the user to customize the order and number of fields (columns) included in the CSV output. By default, the CSV contains the following columns:\n",
    "\n",
    "| Source File Path | Page Number (File) | Page Word Count | Page Text |\n",
    "|:-|:-|:-|:-|\n",
    "|Path/to/PDF/file.pdf|1|6|Text from Page 1 of file.pdf|\n",
    "\n",
    "However, there is other information available, and some users might prefer to include/exclude or arrange their fields differently. to do so, include the `-f` or `--fields` flag, followed by a series of letters indicating the order of fields for the output CSV. These letters should be typed one space after the `-f` flag, put in the desired order for the output fields, and not separated from each other with spaces. For example, `-f nfwtd` gives the same output as above, but with just the file name instead of the full path, and an extra column showing the time it took to process the given page:\n",
    "\n",
    "| Source File Name | Page Number (File) | Page Word Count | Page Text | Page Processing Duration |\n",
    "|:-|:-|:-|:-|:-|\n",
    "|file.pdf|1|6|Text from Page 1 of file.pdf|0.02 sec.|\n",
    "\n",
    "Available fields are:\n",
    "\n",
    "| Indicator | Field Title | Description |\n",
    "|:-|:-|:-|\n",
    "| p | Source File **P**ath | Whole path of the file being processed, eg. /path/to/PDF/1.pdf\n",
    "| n | Source File **N**ame | Just the name of the file being proccesse, eg. 1.pdf\n",
    "| f | Page Number (**F**ile) | The number of the page, starting at 1 for each file\n",
    "| o | Page Number (**O**verall) | The number of the page, continuing for all files\n",
    "| w | **W**ord Count | The number of words extracted from the page\n",
    "| d | Page Processing **D**uration | The amount of seconds taken to process the page\n",
    "| t | **T**ext | The text extracted from the page\n",
    "| r | **R**aw Text | The original text before any processing\n",
    "| s | Process Time**s**tamp | The exact time the page was processed\n",
    "| c | **C**ustom Text | Custom field, see below for details\n",
    "\n",
    "#### Custom Field\n",
    "This option allows users to design a custom field with their own text in it. This is particularly helpful with larger projects where the CSV data will be incorporated into a larger dataset, so the user can define a custom ID for each line. The custom field is included in the output by including `c` in the field flag as outlined above. The title and content for the custom field is set using the flags `-ct` and `-cc` described below.\n",
    "\n",
    "#### Custom Field Title\n",
    "`-ct TITLE` or `--customTitle TITLE`\n",
    "\n",
    "This option allows users to set a custom title for the custom field. Replace `TITLE` in the flag above with the desired title, in quotation marks. \n",
    "\n",
    "#### Custom Field Content\n",
    "`-cc CONTENT` or `--customContent CONTENT`\n",
    "\n",
    "This option allows users to design custom content to appear for each line in the custom field. Replace `CONTENT` in the flag above iwht the desired content in quotation marks.\n",
    "\n",
    "In addition to custom text, including certain special symbol characters will replace those symbols with specific information for each page of the text. These are outlined here:\n",
    "\n",
    "| Symbol | Information | Description |\n",
    "|:-|:-|:-|\n",
    "| # | Page Number (File) | The number of the page, starting at 1 for each file |\n",
    "| $ | Page Number (Overall) | The number of the page, continuing for all files |\n",
    "| @ | Source File Name | Just the name of the file being proccesse, eg. 1.pdf |\n",
    "\n",
    "#### Custom Field Example\n",
    "\n",
    "For example, if a user wanted to process a series of files in the folder `files/PDF`, and include a custom ID to help integrate the data into their larger dataset alongside the text, they could use this command:\n",
    "\n",
    "`%run PDFtoCSV.py \"files/PDF\" -f ct -ct \"ID\" -cc \"Project-7-File-@-p#\"`\n",
    "to generate this output:\n",
    "\n",
    "| ID | Text |\n",
    "|:-|:-|\n",
    "| Project-7-File-Jan2020.pdf-p1 | Text from Page 1 of Jan2020.pdf |\n",
    "| Project-7-File-Jan2020.pdf-p2 | Text from Page 2 of Jan2020.pdf |\n",
    "| Project-7-File-Jan2020.pdf-p3 | Text from Page 3 of Jan2020.pdf |\n",
    "| Project-7-File-Feb2020.pdf-p1 | Text from Page 1 of Feb2020.pdf |\n",
    "| Project-7-File-Feb2020.pdf-p2 | Text from Page 2 of Feb2020.pdf |\n",
    "| Project-7-File-Feb2020.pdf-p3 | Text from Page 3 of Feb2020.pdf |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Reports\n",
    "This program supports the creation of ancillary frequency reports generated alongside the default CSV files. The default files contain the verbatim text from each page of each PDF file, extracted as accurately as possible. The options outlined below create an additional CSV output file containing a frequency report. This report lists all individual words extracted from the PDF files, along with the number of instances of the word (frequency). This frequency report is contained in a file titled the same as the default output CSV file, but with -FR appended to the name. If Split Mode is used, a separate frequency report CSV will be generated for each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Modes\n",
    "\n",
    "#### Report Mode\n",
    "`-r` or `--report`\n",
    "\n",
    "_Cannot be used at the same time as -rf (Report Mode (File)) or -rp (Report Mode(Page))_\n",
    "\n",
    "This creates a freqency report with the cumulative frequencies of all words in all PDF files processed.\n",
    "\n",
    "#### Report Mode (File)\n",
    "`-rf` or `--reportFile`\n",
    "\n",
    "_Cannot be used at the same time as -r (Report Mode) or -rp (Report Mode(Page))_\n",
    "\n",
    "This creates a frequency report with a separate section for each file, with sepearate frequencies per file.\n",
    "\n",
    "#### Report Mode (Page)\n",
    "`rp` or `--reportPage`\n",
    "\n",
    "_Cannot be used at the same time as -r (Report Mode) or -rf (Report Mode (File))_\n",
    "\n",
    "This creates a frequency report with a separate section for each page in each file, with separate frequencies per page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Output Options\n",
    "\n",
    "#### Report Sort Mode\n",
    "`-rs` or `--reportSort`\n",
    "\n",
    "_Must be used in conjuction with -r (Report Mode), -rf (Report Mode (File)), or -rp (Report Mode(Page))_\n",
    "\n",
    "This sorts the entries in the frequency report by frequency, descending, instead of the default alphabetical sorting.\n",
    "\n",
    "#### Report Limit Mode\n",
    "`rl LIMIT` or `--reportLimit LIMIT`\n",
    "\n",
    "_Must be used in conjuction with -r (Report Mode), -rf (Report Mode (File)), or -rp (Report Mode(Page))_\n",
    "\n",
    "This limits the words included in the frequency report to those over a given frequency. Replace `LIMIT` in the flag above with the desired threshold. Numbers given alone represent the mininum number of instances required for a word to be included in the frequency report. Numbers followed by a `%` represent the top percetile of words. For example, `-rl 100` only includes words that appear more than 100 times in the text, while `-rl 70%` includes the words that cumulatively make up 70% of the total words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Reporting Options\n",
    "\n",
    "#### Report Only Mode\n",
    "`-ro [WORDS]` or `--reportOnly [WORDS]`\n",
    "\n",
    "_Cannot be used at the same time as -ri (Report Ignore Mode)_\n",
    "\n",
    "_Must be used in conjuction with -r (Report Mode), -rf (Report Mode (File)), or -rp (Report Mode(Page))_\n",
    "\n",
    "This produces a frequency report that only counts specified words. Replace `[WORDS]` with the words to count, separated by a single space. Eg. `%run PDFtoCSV.py \"example/path/to/folder/with/PDFs\" -r -ro minister government people` will create a frequency report that only contains the frequencies of the words \"minister\", \"government\", and \"people\" from all files in the folder `PDFs`. \n",
    "\n",
    "It is possible to refer to a list of words in a file, in order to avoid typing a long list of words. See [below](#Custom-Selective-Reporting-Word-Lists) for details.\n",
    "\n",
    "#### Report Ignore Mode\n",
    "`-ri [WORDS]` or `--reportIgnore [WORDS]`\n",
    "\n",
    "_Cannot be used at the same time as -ro (Report Only Mode)_\n",
    "\n",
    "_Must be used in conjuction with -r (Report Mode), -rf (Report Mode (File)), or -rp (Report Mode(Page))_\n",
    "\n",
    "This produces a frequency report that counts all words **except** specified words. Replace Replace `[WORDS]` with the words to ignore, separated by a single space. Eg. `%run PDFtoCSV.py \"example/path/to/folder/with/PDFs\" -r -ro the a an but` will create a frequency report that contains the frequencies of all words from all files in the folder `PDFs`, **except** \"the\", \"a\", \"an\", and \"but\".\n",
    "\n",
    "When the `-ri` flag is used without listing any words, it defaults to ignore a large list of common English [stop words](https://en.wikipedia.org/wiki/Stop_words), words that add no meaning to the text, such as \"the\", \"an\", \"but\", and so on. The default list of stop words is taken from [here](http://xpo6.com/list-of-english-stop-words/).\n",
    "\n",
    "It is possible to refer to a list of words in a file, in order to avoid typing a long list of words. See [below](#Custom-Selective-Reporting-Word-Lists) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Selective Reporting Word Lists\n",
    "\n",
    "It is often helpful to have a long list of words to either include or ignore in frequency reports, using the `-ro` or `-ri` [flags](#Selective-Reporting-Options). \n",
    "To facilitate this, there are two text files in the [options](options) folder that can be filled with custom word lists. Using either the `-ro` or `-ri` flags without any words afterward will automatically refer to the words listed in the respective file. By default, [ReportOnly.txt](options/ReportOnly.txt) and [ReportIgnore.txt](options/ReportIgnore.txt) are empty. However, if `-ri` is run without any arguments and without any content in the file, [StopWords.txt](option/StopWords.txt) will be used insted, which is populated by a large [list](http://xpo6.com/list-of-english-stop-words/) of common English stop words. All word list files can be modified and saved for re-use. Note that, once modified, **they will remain that way indefinitely unless changed**. Detailed instructions are available in the files themselves for formatting and use. \n",
    "\n",
    "Alternatively, custom lists of words saved as text files can be used as well. Simply paste the full path to the text file (surrounded by quotation marks) after the flag. Eg. `-ro \"path/to/wordlist.txt\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "This program offers a suite of tools for processing the text between its extraction and output. Raw text is often contains misspelled words and superfluous punctuation from the extraction process and the source text itself. This can affect readability slightly, however, it has the greatest effect on potential analyses of the output. Especially if you intend to use this data for Natual Language Processing or another form of large-scale analysis, please consider the appropriate options below to create to mose useful output for you. Note that for large projects it is advisable to experiment with various options on a representative sample of the source data to ascertain the most useful options.\n",
    "\n",
    "#### Raw Mode\n",
    "\n",
    "`-pr` or `--processRaw`\n",
    "\n",
    "This creates a separate column in the CSV output with the raw, unedited text extracted from the source. This can then be compared against the processed text to determine the efficacy of the processing options employed. By default it is the last column in the CSV file, but can be rearranged using the `r` indicator in the `-f` option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction Options\n",
    "\n",
    "#### Autocorrect Mode\n",
    "`-pa` or `--processAutocorrect`\n",
    "\n",
    "This runs each word extracted from the source text through a spell checker, and replaces misspelled words with the most likely correction when possible. The program uses a default dictionary to check spelling, with approximately 70% accuracy. Please see Dictionary Options below to learn more about creating specialized spelling dictionaries for the topic of your project.\n",
    "\n",
    "#### Corrections Mode\n",
    "`-pc` or `--processCorrections`\n",
    "\n",
    "_Must be used in conjuction with -pa (Autocorrect Mode)_\n",
    "\n",
    "This option creates a separate file containing all of the words that were flagged as misspelled in the original text when processed in Autocorrect Mode. It specifies whether a suitable correction was used or not, and if so, the confidence that the correction is the most appropriate one, on a scale of 0-1. This file is saved in the same location as the CSV output and Frequency Report files, in a file titled the same as the default output CSV file, but with -Corrections appended to the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dictionary Options\n",
    "\n",
    "#### Dictionary Mode\n",
    "\n",
    "`-pd [topic(s)]` or `--processDictionary [topic(s)]`\n",
    "\n",
    "This mode creates a custom dictionary specialized for a given topic or topics. This dictionary is then used by default to check spelling and offer corrections when using the `-pa` option (Autocorrect Mode) or the `-pw` option (Process Words Mode). \n",
    "\n",
    "This process uses pages from [Wikipedia](https://en.wikipedia.org) to populate the dictionary and calculate the frequency of words. Topics passed to this mode should correspond to the titles of known Wikipedia articles on major subjects. For each keyword submitted, the program will gather the text of that page, along with the text of every Wikipeida page linked by that page. This large amount of text is then processed to create the custom dictionary. For example, if the project is analyzing PDF news sources about a communist insurgency in the Phillipines, the command could look like `%run PDFtoCSV.py source.pdf -pd Philippines \"New People's Army\" \"Communism in the Philippines\"`. This command gathers text from over 1100 pages related to the topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Common Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please enter a valid file or directory / The folder you have entered does not contain any PDF files\n",
    "* Include the full and exact spelling and syntax of the path to your file or folder. The best way to ensure this is to copy it directly, here's how:\n",
    "    * Open the folder containing the folder or file you want to copy\n",
    "    * On Windows:\n",
    "      * Hold Shift and right click on the folder or file\n",
    "      * Left click on the menu option \"Copy as Path\"\n",
    "    * On MacOS\n",
    "      * Right click on the folder or file\n",
    "      * Hold the Option button\n",
    "      * Left click on the menu option \"Copy [name] as Pathname\"\n",
    "    * Paste in the cell above, replacing `\"path/to/folder/with/PDF\"`\n",
    "    * Be sure to put a quotation mark before **and** after the path you have pasted\n",
    "    * Press Shift + Enter to run the program  \n",
    "    \n",
    "### There appears to be an issue with your Tesseract OCR installation\n",
    "* Refer to 1. under the \"Installation\" section of this guide on how to install Tesseract OCR. If you have it installed, try uninstalling and re-installing, being sure to accept all default options on the installation. \n",
    "\n",
    "### I could not find your Tesseract-OCR installation in the default location.\n",
    "* Refer to 1. under the \"Installation section of this guide on how to install Tesseract OCR.\n",
    "* Open the file `tesseractPath.txt` in the same folder as this file, and follow the instructions there to locate your Tesseract program on Linux, or if it was installed in a non-default location on Windows or MacOS.\n",
    "\n",
    "### Other issues\n",
    "This program is still in very early development, and at this point is not fully complete with robust error handling or options for non-standard situations. We attempted to make it as broadly usable as possible. Hopefully further versions will come that include more detailed options for handling unexpected issues. \n",
    "\n",
    "Please feel free to contact me at danieljwhitten@gmail.com, if you have any small issues that I might easily be able to provide advice on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
